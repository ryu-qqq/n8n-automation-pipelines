{
  "name": "AI Agent Advanced Crawler",
  "nodes": [
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "assignment-1",
              "name": "config",
              "value": "{\n  \"type\": \"html\",\n  \"url\": \"https://quotes.toscrape.com\",\n  \"extractors\": {\n    \"quotes\": {\n      \"selector\": \".quote\",\n      \"fields\": {\n        \"text\": \".text::text\",\n        \"author\": \".author::text\",\n        \"tags\": \".tag::text[]\"\n      }\n    }\n  },\n  \"options\": {\n    \"maxResults\": 100,\n    \"chunkSize\": 20,\n    \"timeout\": 10000\n  }\n}",
              "type": "object"
            }
          ]
        },
        "options": {}
      },
      "id": "set-config",
      "name": "Set Crawler Config",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        160,
        240
      ],
      "notes": "í¬ë¡¤ëŸ¬ ì„¤ì • JSON:\n\nğŸ“‹ HTML í¬ë¡¤ë§ ì˜ˆì‹œ:\n{\n  \"type\": \"html\",\n  \"url\": \"https://quotes.toscrape.com\",\n  \"extractors\": {\n    \"quotes\": {\n      \"selector\": \".quote\",\n      \"fields\": {\n        \"text\": \".text::text\",\n        \"author\": \".author::text\"\n      }\n    }\n  },\n  \"options\": {\n    \"maxResults\": 100,\n    \"chunkSize\": 20\n  }\n}\n\nğŸ”Œ API í¬ë¡¤ë§ ì˜ˆì‹œ:\n{\n  \"type\": \"api\",\n  \"url\": \"https://jsonplaceholder.typicode.com/posts\",\n  \"extractors\": {\n    \"posts\": {\n      \"path\": \"$\",\n      \"fields\": {\n        \"title\": \"$.title\",\n        \"body\": \"$.body\",\n        \"userId\": \"$.userId\"\n      }\n    }\n  },\n  \"options\": {\n    \"maxResults\": 50,\n    \"chunkSize\": 10\n  }\n}"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "condition-1",
              "leftValue": "={{ $json.config.type }}",
              "rightValue": "api",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "check-type",
      "name": "Check Crawler Type",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        380,
        240
      ]
    },
    {
      "parameters": {
        "url": "={{ $json.config.url }}",
        "options": {
          "timeout": "={{ $json.config.options?.timeout || 10000 }}"
        },
        "headers": {
          "User-Agent": "Mozilla/5.0 (compatible; n8n-crawler/1.0)",
          "Accept": "application/json"
        }
      },
      "id": "api-request",
      "name": "API Request",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        600,
        160
      ]
    },
    {
      "parameters": {
        "url": "={{ $json.config.url }}",
        "options": {
          "timeout": "={{ $json.config.options?.timeout || 10000 }}"
        },
        "headers": {
          "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
      },
      "id": "html-request",
      "name": "HTML Request",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        600,
        320
      ]
    },
    {
      "parameters": {
        "jsCode": "// API ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬ ë° ì¶”ì¶œ\nconst items = $input.all();\nconst results = [];\n\nfor (const item of items) {\n  const config = item.json.config;\n  const responseData = item.json.data || item.json;\n  \n  // ì¶”ì¶œê¸° ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n  const extractors = config.extractors || {};\n  const options = config.options || {};\n  const maxResults = options.maxResults || 1000;\n  const chunkSize = options.chunkSize || 50;\n  \n  const extractedData = {};\n  \n  // ê° ì¶”ì¶œê¸°ì— ëŒ€í•´ ì²˜ë¦¬\n  for (const [extractorName, extractorConfig] of Object.entries(extractors)) {\n    try {\n      let targetData = responseData;\n      \n      // JSONPathë‚˜ ê°ì²´ ê²½ë¡œë¡œ ë°ì´í„° ì ‘ê·¼\n      if (extractorConfig.path) {\n        const path = extractorConfig.path;\n        if (path === '$') {\n          targetData = responseData;\n        } else if (path.startsWith('$.')) {\n          // ê°„ë‹¨í•œ JSONPath ì²˜ë¦¬\n          const keys = path.substring(2).split('.');\n          for (const key of keys) {\n            if (targetData && targetData[key] !== undefined) {\n              targetData = targetData[key];\n            } else {\n              targetData = null;\n              break;\n            }\n          }\n        }\n      }\n      \n      if (!targetData) {\n        extractedData[extractorName] = [];\n        continue;\n      }\n      \n      // ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜\n      const dataArray = Array.isArray(targetData) ? targetData : [targetData];\n      \n      // í•„ë“œ ì¶”ì¶œ\n      const extractedItems = [];\n      for (let i = 0; i < Math.min(dataArray.length, maxResults); i++) {\n        const dataItem = dataArray[i];\n        const extractedItem = {};\n        \n        if (extractorConfig.fields) {\n          for (const [fieldName, fieldPath] of Object.entries(extractorConfig.fields)) {\n            try {\n              if (fieldPath.startsWith('$.')) {\n                // JSONPath ì²˜ë¦¬\n                const keys = fieldPath.substring(2).split('.');\n                let value = dataItem;\n                for (const key of keys) {\n                  if (value && value[key] !== undefined) {\n                    value = value[key];\n                  } else {\n                    value = null;\n                    break;\n                  }\n                }\n                extractedItem[fieldName] = value;\n              } else {\n                // ì§ì ‘ í•„ë“œëª…\n                extractedItem[fieldName] = dataItem[fieldPath] || null;\n              }\n            } catch (error) {\n              extractedItem[fieldName] = null;\n            }\n          }\n        } else {\n          // í•„ë“œ ì„¤ì •ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°\n          extractedItem.data = dataItem;\n        }\n        \n        extractedItems.push(extractedItem);\n      }\n      \n      extractedData[extractorName] = extractedItems;\n      \n    } catch (error) {\n      extractedData[extractorName] = {\n        error: error.message,\n        type: 'extraction_error'\n      };\n    }\n  }\n  \n  // ê²°ê³¼ ì²­í‚¹\n  const chunks = [];\n  for (const [extractorName, data] of Object.entries(extractedData)) {\n    if (Array.isArray(data)) {\n      for (let i = 0; i < data.length; i += chunkSize) {\n        chunks.push({\n          extractor: extractorName,\n          chunk_index: Math.floor(i / chunkSize),\n          total_chunks: Math.ceil(data.length / chunkSize),\n          total_items: data.length,\n          items: data.slice(i, i + chunkSize),\n          config: {\n            url: config.url,\n            type: config.type,\n            timestamp: new Date().toISOString()\n          }\n        });\n      }\n    } else {\n      chunks.push({\n        extractor: extractorName,\n        chunk_index: 0,\n        total_chunks: 1,\n        total_items: 1,\n        items: [data],\n        config: {\n          url: config.url,\n          type: config.type,\n          timestamp: new Date().toISOString()\n        }\n      });\n    }\n  }\n  \n  results.push(...chunks);\n}\n\nreturn results.map(result => ({ json: result }));"
      },
      "id": "process-api",
      "name": "Process API Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        820,
        160
      ]
    },
    {
      "parameters": {
        "dataPropertyName": "html_content",
        "extractionValues": {
          "values": []
        }
      },
      "id": "html-extract",
      "name": "Extract HTML",
      "type": "n8n-nodes-base.htmlExtract",
      "typeVersion": 1,
      "position": [
        820,
        320
      ]
    },
    {
      "parameters": {
        "jsCode": "// HTML ì¶”ì¶œ ê²°ê³¼ ì²˜ë¦¬ ë° ì²­í‚¹\nconst items = $input.all();\nconst results = [];\n\nfor (const item of items) {\n  const config = item.json.config;\n  const htmlContent = item.json.data;\n  \n  // cheerio ìŠ¤íƒ€ì¼ íŒŒì‹± ì‹œë®¬ë ˆì´ì…˜\n  const extractors = config.extractors || {};\n  const options = config.options || {};\n  const maxResults = options.maxResults || 1000;\n  const chunkSize = options.chunkSize || 50;\n  \n  const extractedData = {};\n  \n  // HTML íŒŒì‹±ì„ ìœ„í•œ ë”ë¯¸ ë°ì´í„° (ì‹¤ì œë¡œëŠ” HTML Extract ë…¸ë“œì—ì„œ ì²˜ë¦¬)\n  // ì´ ë¶€ë¶„ì€ HTML Extract ë…¸ë“œì˜ ì„¤ì •ì— ë”°ë¼ ê²°ì •ë¨\n  \n  for (const [extractorName, extractorConfig] of Object.entries(extractors)) {\n    try {\n      // HTML Extract ë…¸ë“œì—ì„œ ì¶”ì¶œëœ ë°ì´í„° ì‚¬ìš©\n      const extractedItems = item.json[extractorName] || [];\n      \n      // ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜\n      const dataArray = Array.isArray(extractedItems) ? extractedItems : [extractedItems];\n      \n      // ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ\n      const limitedData = dataArray.slice(0, maxResults);\n      \n      extractedData[extractorName] = limitedData;\n      \n    } catch (error) {\n      extractedData[extractorName] = {\n        error: error.message,\n        type: 'html_extraction_error'\n      };\n    }\n  }\n  \n  // ê²°ê³¼ ì²­í‚¹\n  const chunks = [];\n  for (const [extractorName, data] of Object.entries(extractedData)) {\n    if (Array.isArray(data)) {\n      for (let i = 0; i < data.length; i += chunkSize) {\n        chunks.push({\n          extractor: extractorName,\n          chunk_index: Math.floor(i / chunkSize),\n          total_chunks: Math.ceil(data.length / chunkSize),\n          total_items: data.length,\n          items: data.slice(i, i + chunkSize),\n          config: {\n            url: config.url,\n            type: config.type,\n            timestamp: new Date().toISOString()\n          }\n        });\n      }\n    } else {\n      chunks.push({\n        extractor: extractorName,\n        chunk_index: 0,\n        total_chunks: 1,\n        total_items: 1,\n        items: [data],\n        config: {\n          url: config.url,\n          type: config.type,\n          timestamp: new Date().toISOString()\n        }\n      });\n    }\n  }\n  \n  results.push(...chunks);\n}\n\nreturn results.map(result => ({ json: result }));"
      },
      "id": "process-html",
      "name": "Process HTML Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1040,
        320
      ]
    },
    {
      "parameters": {
        "mode": "combine",
        "combinationMode": "mergeByPosition",
        "options": {}
      },
      "id": "merge-results",
      "name": "Merge Results",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        1260,
        240
      ]
    },
    {
      "parameters": {
        "jsCode": "// ìµœì¢… ê²°ê³¼ í¬ë§¤íŒ… ë° ìš”ì•½\nconst items = $input.all();\nconst results = [];\n\n// ì²­í¬ë³„ í†µê³„ ê³„ì‚°\nconst stats = {\n  total_chunks: items.length,\n  total_items: 0,\n  extractors: {},\n  processing_time: new Date().toISOString()\n};\n\nfor (const item of items) {\n  const data = item.json;\n  \n  stats.total_items += data.total_items || 0;\n  \n  if (!stats.extractors[data.extractor]) {\n    stats.extractors[data.extractor] = {\n      chunks: 0,\n      items: 0\n    };\n  }\n  \n  stats.extractors[data.extractor].chunks++;\n  stats.extractors[data.extractor].items += (data.items || []).length;\n  \n  // AI ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§¤íŒ…\n  const formattedResult = {\n    chunk_id: `${data.extractor}_chunk_${data.chunk_index}`,\n    extractor: data.extractor,\n    chunk_info: {\n      index: data.chunk_index,\n      total_chunks: data.total_chunks,\n      items_in_chunk: (data.items || []).length,\n      total_items: data.total_items\n    },\n    data: data.items || [],\n    metadata: {\n      source_url: data.config?.url,\n      crawler_type: data.config?.type,\n      extracted_at: data.config?.timestamp\n    }\n  };\n  \n  results.push(formattedResult);\n}\n\n// ì²« ë²ˆì§¸ ê²°ê³¼ì— ì „ì²´ í†µê³„ ì¶”ê°€\nif (results.length > 0) {\n  results[0].crawl_summary = stats;\n}\n\nreturn results.map(result => ({ json: result }));"
      },
      "id": "format-output",
      "name": "Format for AI Agent",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1480,
        240
      ]
    },
    {
      "parameters": {
        "content": "## ğŸ¤– AI ì—ì´ì „íŠ¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ\n\n### ğŸ“Š í¬ë¡¤ë§ ìš”ì•½\n- **URL**: {{ $json.metadata?.source_url }}\n- **í¬ë¡¤ëŸ¬ íƒ€ì…**: {{ $json.metadata?.crawler_type }}\n- **ì²˜ë¦¬ ì‹œê°„**: {{ $json.metadata?.extracted_at }}\n\n{% if $json.crawl_summary %}\n### ğŸ“ˆ ì „ì²´ í†µê³„\n- **ì´ ì²­í¬ ìˆ˜**: {{ $json.crawl_summary.total_chunks }}\n- **ì´ ì•„ì´í…œ ìˆ˜**: {{ $json.crawl_summary.total_items }}\n\n**ì¶”ì¶œê¸°ë³„ í†µê³„**:\n{% for extractor, data in $json.crawl_summary.extractors %}\n- **{{ extractor }}**: {{ data.items }}ê°œ ì•„ì´í…œ ({{ data.chunks }}ê°œ ì²­í¬)\n{% endfor %}\n{% endif %}\n\n### ğŸ“¦ í˜„ì¬ ì²­í¬ ì •ë³´\n- **ì²­í¬ ID**: {{ $json.chunk_id }}\n- **ì¶”ì¶œê¸°**: {{ $json.extractor }}\n- **ì²­í¬ ë²ˆí˜¸**: {{ $json.chunk_info.index + 1 }} / {{ $json.chunk_info.total_chunks }}\n- **ì´ ì²­í¬ì˜ ì•„ì´í…œ ìˆ˜**: {{ $json.chunk_info.items_in_chunk }}\n\n### ğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 3ê°œ)\n```json\n{{ JSON.stringify($json.data.slice(0, 3), null, 2) }}\n```\n\n---\n*ì´ ë°ì´í„°ëŠ” AI ì—ì´ì „íŠ¸ì˜ íˆ´ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í¬ë§¤íŒ…ë˜ì—ˆìŠµë‹ˆë‹¤.*",
        "options": {}
      },
      "id": "ai-report",
      "name": "AI Agent Report",
      "type": "n8n-nodes-base.html",
      "typeVersion": 1,
      "position": [
        1700,
        240
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Set Crawler Config": {
      "main": [
        [
          {
            "node": "Check Crawler Type",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Crawler Type": {
      "main": [
        [
          {
            "node": "API Request",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "HTML Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "API Request": {
      "main": [
        [
          {
            "node": "Process API Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTML Request": {
      "main": [
        [
          {
            "node": "Extract HTML",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process API Data": {
      "main": [
        [
          {
            "node": "Merge Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract HTML": {
      "main": [
        [
          {
            "node": "Process HTML Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process HTML Data": {
      "main": [
        [
          {
            "node": "Merge Results",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Results": {
      "main": [
        [
          {
            "node": "Format for AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format for AI Agent": {
      "main": [
        [
          {
            "node": "AI Agent Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "2.0.0",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "ai-agent-advanced-crawler"
  },
  "id": "ai-agent-advanced-crawler",
  "tags": [
    {
      "createdAt": "2025-07-14T05:20:00.000Z",
      "updatedAt": "2025-07-14T05:20:00.000Z",
      "id": "ai-agent",
      "name": "ai-agent"
    },
    {
      "createdAt": "2025-07-14T05:20:00.000Z",
      "updatedAt": "2025-07-14T05:20:00.000Z",
      "id": "advanced-crawler",
      "name": "advanced-crawler"
    },
    {
      "createdAt": "2025-07-14T05:20:00.000Z",
      "updatedAt": "2025-07-14T05:20:00.000Z",
      "id": "chunking",
      "name": "chunking"
    }
  ]
}
